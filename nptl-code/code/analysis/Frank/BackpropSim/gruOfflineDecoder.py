#Todo:
#dropout layer, noise injection
#stacked GRU nets
#layer normalization
#hyperparameter search

#Apply a GRU network offline
import scipy.io
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from rnnUtils import GRU, initializeWeights

#Input & Targets generated by MATLAB
dataDir = '/Users/frankwillett/Data/Derived/gruTest'
rnnData = scipy.io.loadmat(dataDir + '/gruInput')

inputs = rnnData['inputs'].astype(np.float32)
targets = rnnData['targets'].astype(np.float32)
inputsVal = rnnData['inputsVal'].astype(np.float32)
targetsVal = rnnData['targetsVal'].astype(np.float32)
inputsFinal = rnnData['inputsFinal'].astype(np.float32)
targetsFinal = rnnData['targetsFinal'].astype(np.float32)

nDecFactors = 10
nDecUnits = 50
dt = 0.02
batchSize = 32
nInputs = inputs.shape[2]
nSteps = inputs.shape[1]
nTargets = targets.shape[2]
nTrials = inputs.shape[0]
nValTrials = inputsVal.shape[0]

#Start tensorflow
sess = tf.Session()

#these placeholders must be configured for each new batch
batchInputs = tf.placeholder(tf.float32, shape=[nInputs, nSteps, batchSize])
batchTargets = tf.placeholder(tf.float32, shape=[nTargets, nSteps, batchSize])
startDecState = tf.placeholder(tf.float32, shape=[nDecUnits, batchSize])

#instantiate a decoder GRU network
#reset_bias = 1.0, update_bias = -1.0
decNetwork = GRU(nDecUnits, nDecFactors, nTargets, 'GRU', reset_bias=1.0, update_bias=-1.0, weight_scale=1.0,
               clip_value=np.inf)

#create a linear projection layer on the inputs
with tf.variable_scope('ProjLayer'):
    projW = tf.get_variable("W", dtype=tf.float32, 
            initializer=initializeWeights([nDecFactors, nInputs ], 1.0), trainable=True)
    projB = tf.get_variable("b", [nDecFactors, 1], dtype=tf.float32, 
            initializer=tf.zeros_initializer, trainable=True)
    
#unfold RNN in time
decStates = [startDecState]
decOutputs = []
smoothOutputs = [tf.zeros([nTargets, 1])]
inputFactors = []
zGates = []
rGates = []

for i in range(nSteps):   
    #compress input
    inputProj = 1*tf.matmul(projW, batchInputs[:,i,:]) + projB
    
    #step forward the RNN
    newState, newOutput, newZ, newR = decNetwork(inputProj, decStates[i])
    newOutput = newOutput*1
    
    #smooth the decoded output
    smoothOutputs.append(0.8*smoothOutputs[i] + 0.2*newOutput)
    
    #append state & output to the chain
    decStates.append(newState)
    decOutputs.append(newOutput)
    inputFactors.append(inputProj)
    zGates.append(newZ)
    rGates.append(newR)

    #compute error for time steps greater than som burn-in time
    if i>100:
        tf.add_to_collection('SquaredErr',tf.square(newOutput-batchTargets[:,i,:]))

#compute total error for training
totalErr = (tf.reduce_sum(tf.add_n(tf.get_collection('SquaredErr'), name='total_err'))/(batchSize*(nSteps-101)*nTargets))
rmse = tf.sqrt(totalErr)
trainErrSummary = tf.summary.scalar('train_RMSE', rmse)
testErrSummary = tf.summary.scalar('test_RMSE', rmse)

#hidden state derivative cost
hsDerivCost = tf.constant(0.0)
for i in range(len(decOutputs)-1):
    hsDerivCost += tf.reduce_sum(tf.square(decOutputs[i+1]-decOutputs[i]))
hsDerivCost = hsDerivCost / ((len(decOutputs)-1)*batchSize)
hsDerivCost = hsDerivCost / (tf.reduce_sum(tf.square(decOutputs)) / ((len(decOutputs)-1)*batchSize))
hsDerivCost = hsDerivCost * 0
hsDerivSummary = tf.summary.scalar('derivCost', hsDerivCost)

#add l2 cost
l2vars = [decNetwork.W, decNetwork.U, decNetwork.W_z, decNetwork.U_z, decNetwork.W_r, decNetwork.U_r, decNetwork.W_o, projW]
l2cost = tf.constant(0.0)
total_params = 0
for i in range(len(l2vars)):
  shape = l2vars[i].get_shape().as_list()
  total_params += np.prod(shape)
  l2cost += tf.nn.l2_loss(l2vars[i])
l2cost = l2cost / total_params
l2cost = l2cost * 0
l2costSummary = tf.summary.scalar('l2cost', l2cost)

#total cost
totalCost = hsDerivCost + l2cost + totalErr

#prepare gradients and optimizer
tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
learnRate = tf.Variable(1.0, trainable=False)

grads = tf.gradients(totalCost, tvars)
grads, grad_global_norm = tf.clip_by_global_norm(grads, 200)
opt = tf.train.AdamOptimizer(learnRate, beta1=0.9, beta2=0.999,
                             epsilon=1e-01)
train_op = opt.apply_gradients(
    zip(grads, tvars), global_step=tf.contrib.framework.get_or_create_global_step())
    
new_lr = tf.placeholder(tf.float32, shape=[], name="new_learning_rate")
lr_update = tf.assign(learnRate, new_lr)

#prepare tensorboard
writer = tf.summary.FileWriter("/Users/frankwillett/Data/Derived/gruTest")

#prepare to save the model
saver = tf.train.Saver()
lve = np.inf

#How many parameters does this model have?
total_params = 0
for i in range(len(tvars)):
  shape = tvars[i].get_shape().as_list()
  total_params += np.prod(shape)
print("Total model parameters: ", total_params)

sess.run(tf.global_variables_initializer())

#train RNN one batch at a time
nBatches = 2000
prevStartStates = np.zeros([nDecUnits, batchSize])

for i in range(nBatches):
    #learn rate
    lr = 0.01*(1 - i/float(nBatches))
    
    #prepare to insert inputs and targets from randomly selected trials
    trlIdx = np.random.choice(nTrials, batchSize)
    
    #random start state and target
    inputSeq = np.transpose(inputs[trlIdx,:,:], (2,1,0))
    targSeq = np.transpose(targets[trlIdx,:,:], (2,1,0))
    
    #descend gradient
    ao, to, te, trainSummary, hsSumm, l2Summ = sess.run([lr_update, train_op, totalErr, trainErrSummary, hsDerivSummary, l2costSummary], 
                                        feed_dict={new_lr: lr, startDecState: prevStartStates, batchInputs: inputSeq, batchTargets: targSeq})

    #log progress
    writer.add_summary(trainSummary, i)
    writer.add_summary(hsSumm, i)
    writer.add_summary(l2Summ, i)
    
    #track validation accuracy on a random validation batch
    valIdx = np.random.choice(nValTrials, batchSize)
    inputSeq = np.transpose(inputsVal[valIdx,:,:], (2,1,0))
    targSeq = np.transpose(targetsVal[valIdx,:,:], (2,1,0))
    te, testSummary = sess.run([totalErr, testErrSummary], feed_dict={new_lr: lr, startDecState: prevStartStates, batchInputs: inputSeq, batchTargets: targSeq})
    writer.add_summary(testSummary, i)
    
    #save whenever validation error is at its lowest point so far
    if te < lve:
        lve = te
        saver.save(sess, '/Users/frankwillett/Data/Derived/gruTest/model.ckpt', global_step=i, write_meta_graph=False)
    
    #validation plot every once in a while
    if i%100 == 0:
        print("step %d"%(i))
        
        inputSeq = np.transpose(inputsVal[range(batchSize),:,:], (2,1,0))
        targSeq = np.transpose(targetsVal[range(batchSize),:,:], (2,1,0))
        do, inf, ds = sess.run([decOutputs, inputFactors, decStates], feed_dict={new_lr: lr, startDecState: prevStartStates, batchInputs: inputSeq, batchTargets: targSeq})
        do = np.stack(do)
        inf = np.stack(inf)
        ds = np.stack(ds)
        
        plt.figure()
        for x in range(3):
            plt.subplot(3,2,x*2+1)
            plt.plot(targetsVal[x,:,0])
            plt.plot(do[:,0,x])
            
            plt.subplot(3,2,x*2+2)
            plt.plot(targetsVal[x,:,1])
            plt.plot(do[:,1,x])
        plt.show()

#load the best performing variables
ckpt = tf.train.get_checkpoint_state('/Users/frankwillett/Data/Derived/gruTest/')
saver.restore(sess, ckpt.model_checkpoint_path)

#apply to inputsFinal and return
finalIdx = np.array(range(batchSize))
outputs = []
inFac = []
zList = []
rList = []
dsList = []
while True:
    finalIdx = finalIdx[finalIdx<inputsFinal.shape[0]]
    if len(finalIdx)==0:
        break
    if len(finalIdx)<batchSize:
        finalIdx = np.concatenate((finalIdx, np.zeros([batchSize-len(finalIdx)])+finalIdx[-1])).astype(np.int32)
    inputSeq = np.transpose(inputsFinal[finalIdx,:,:], (2,1,0))
    targSeq = np.transpose(targetsFinal[finalIdx,:,:], (2,1,0))

    do, infc, ds, zg, rg = sess.run([decOutputs, inputFactors, decStates, zGates, rGates], feed_dict={new_lr: lr, startDecState: prevStartStates, batchInputs: inputSeq, batchTargets: targSeq})
    outputs.append(np.stack(do))
    inFac.append(np.stack(infc))
    dsList.append(np.stack(ds))
    zList.append(np.stack(zg))
    rList.append(np.stack(rg))
    finalIdx = finalIdx+batchSize

inputSeq = np.transpose(inputsFinal[range(batchSize),:,:], (2,1,0))
targSeq = np.transpose(targetsFinal[range(batchSize),:,:], (2,1,0))
do = sess.run(decOutputs, feed_dict={new_lr: lr, startDecState: prevStartStates, batchInputs: inputSeq, batchTargets: targSeq})
do = np.stack(do)

plt.figure()
for x in range(3):
    plt.subplot(3,2,x*2+1)
    plt.plot(targetsFinal[x,:,0])
    plt.plot(do[:,0,x])
    
    plt.subplot(3,2,x*2+2)
    plt.plot(targetsFinal[x,:,1])
    plt.plot(do[:,1,x])
plt.show()

outputsFinal = np.concatenate(outputs,2)
outputsFinal = outputsFinal[:,:,range(inputsFinal.shape[0])]
outputsFinal = np.transpose(outputsFinal, [2, 0, 1])

inFacFinal = np.concatenate(inFac,2)
inFacFinal = inFacFinal[:,:,range(inputsFinal.shape[0])]
inFacFinal = np.transpose(inFacFinal, [2, 0, 1])

dsFinal = np.concatenate(dsList,2)
dsFinal = dsFinal[:,:,range(inputsFinal.shape[0])]
dsFinal = np.transpose(dsFinal, [2, 0, 1])

zFinal = np.concatenate(zList,2)
zFinal = zFinal[:,:,range(inputsFinal.shape[0])]
zFinal = np.transpose(zFinal, [2, 0, 1])

rFinal = np.concatenate(rList,2)
rFinal = rFinal[:,:,range(inputsFinal.shape[0])]
rFinal = np.transpose(rFinal, [2, 0, 1])
                
a = {}
a['targetsFinal']=targetsFinal
a['outputsFinal']=outputsFinal
a['inFacFinal']=inFacFinal
a['dsFinal']=dsFinal
a['zFinal']=zFinal
a['rFinal']=rFinal
scipy.io.savemat('/Users/frankwillett/Data/Derived/gruTest/gruResults',a)